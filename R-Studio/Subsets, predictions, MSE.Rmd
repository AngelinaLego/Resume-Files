---
title: "Problem Set 8"
author: "Angelina Legostaeva"
date: "2025-03-28"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


####################################################################################

1. G-stat - poisson
2. weird comupations - limits

3. set-sead
4. fit model
5. predict
6. MSE

7. t.test
8. subsets
9. plott.test


####################################################################################


### 1. Using the data set `epl1819.csv`, create a variable

1. Data:
```{r}
epl <- read.csv("epl1819.csv")

TotalGoals <- epl$FTHG + epl$FTAG
table(TotalGoals)

n <- length(TotalGoals)
```
####################################################################################

G-statistic:
```{r}
lambda.hat <- mean(TotalGoals)
lambda.hat

p <- dpois(0:6, lambda.hat)

exp <- n * p
expected <- c(exp, n - sum(exp))

observed <- c(22, 55, 99, 84, 65, 32, 16, 7)
  
G <- 2 * sum(observed * log(observed / expected))
G

p_value <- 1 - pchisq(G, 6)
p_value
```
H0 : TotalGoals follows a Poisson distribution
H1 : TotalGoals does not follow a Poisson distribution

Conclusion: p-value is 0.7, which greater than 0.5.
So, failing to reject the null hypothesis.
Therefore, TotalGoals follows a Poisson distribution.

####################################################################################

2. 

H0 : the data follows a Negative Binomial distribution
H1 : the data does not follow a Negative Binomial distribution

Data:
```{r}
hurricanes <- read.csv("hurricanes(3).csv")

alld <- hurricanes$alldeaths
table(alld)

n <- length(alld)
```

(a) Using the raw data, fill in the “Times observed” column.
```{r}
g1 <- sum(alld == 0)
g2 <- sum(alld >= 1 & alld <= 4)
g3 <- sum(alld >= 5 & alld <= 9)
g4 <- sum(alld >= 10 & alld <= 14)
g5 <- sum(alld >= 15 & alld <= 19)
g6 <- sum(alld >= 20 & alld <= 29)
g7 <- sum(alld >= 30 & alld <= 49)
g8 <- sum(alld >= 50 & alld <= 74)
g9 <- sum(alld >= 72)

observed <- c(g1, g2, g3, g4, g5, g6, g7, g8, g9)
observed
```


(b) Using the MLEs and R functions such as dnbinom() and pnbinom(), fill in the “Time
expected” column.

$par
0.44617466 0.02114409
```{r}
n <- length(alld)
r <- 0.44617466
p <- 0.02114409

exp1 <- dnbinom(0, r, p)
exp2 <- pnbinom(4, r, p) - pnbinom(0, r, p)
exp3 <- pnbinom(9, r, p) - pnbinom(4, r, p)
exp4 <- pnbinom(14, r, p) - pnbinom(9, r, p)
exp5 <- pnbinom(19, r, p) - pnbinom(14, r, p)
exp6 <- pnbinom(29, r, p) - pnbinom(19, r, p)
exp7 <- pnbinom(49, r, p) - pnbinom(29, r, p)
exp8 <- pnbinom(74, r, p) - pnbinom(49, r, p)
exp9 <- 1 - pnbinom(74, r, p)

expected <- c(exp1, exp2, exp3, exp4, exp5, exp6, exp7, exp8, exp9)
exp <- n * expected
exp
```

(c) Calculate the G-statistic for a likelihood ratio test.
```{r}
G <- 2 * sum((observed * log(observed/exp)))
G 
```

(d) 

```{r}
#  n - 1 - # of params
# 9 - 1 - 2 = 6

p_val <- 1 - pchisq(G, 6)
p_val
```
Conclusion: p-value is 0.004 which is less than 0.05.
So, rejecting the null hypothesis, 
the data is not compatible with the negative binomial


####################################################################################


3. Data:
```{r}
NFL <- read.csv("NFL.csv")

NFLclean <- NFL[complete.cases(NFL$wind),]
```


(a) Fit a simple linear regression model on the cleaned data using wind to predict total (Model 1.) 


total = 45.3645 - (0.2547 * wind) + error


H0 : wind does help to predict total points scored

H1 : wind does NOT help to predict total points scored

```{r}
total.lm <- lm(total ~ wind, data = NFLclean)

summary(total.lm)
AIC(total.lm)
```
Conclusion: P-value is 4.284e-11, which is smaller than 0.05.
I am rejecting the null hypothesis that wind does help to predict total points scored.
The evidence are for the alternative hypothesis.


(b) Fit a simple linear regression model on the cleaned data using total line to predict
total (Model 2.) 

total = 6.03504 - (0.87101 * total_line) + error

H0 : total line will give much more accurate predictions than Model 1.
H1 : total line will not give much more accurate predictions than Model 1


```{r}
total_line.lm <- lm(total ~ total_line, data = NFLclean)

summary(total_line.lm)
AIC(total_line.lm)
```
Conclusion: P-value is 2.2e-16, which is smaller than 0.05.
So, I am rejecting the null hypothesis.
Total line will not give much more accurate predictions than Model 1


(c) Fit a multiple linear regression model on the cleaned data using both total line and
wind to predict total (Model 3.) Write down an equation for the model. Find the AICs
for Models 1, 2, and 3. Which model gives the lowest AIC?

total = 8.4219 + (0.8534 * total_line) - (0.1921 * wind) + error

H0 : both total line and wind will give much more accurate prediction for total
H1 : both total line and wind will not give much more accurate prediction for total

```{r}
both.lm <- lm(total ~ total_line + wind, data = NFLclean)
both.lm

summary(both.lm)
AIC(both.lm)
```
Conclusion: P-value is 2.2e-16, which is smaller than 0.05.
So, I am rejecting the null hypothesis.
Both total line and wind will not give much more accurate prediction for total.


Third model with both total line and wind gives the lowest AIC (36775.24).

####################################################################################
####################################################################################

(d)

```{r}
set.seed(352)

n <- nrow(NFLclean)

sample(n, 3200)
NFLclean.rand <- NFLclean[sample(n), ]
```


Fit some models:
```{r}
#n * 0.7 #Do I need this line?

NFLclean.train <- NFLclean.rand[1:324, ]
NFLclean.test <- NFLclean.rand[325:463, ]

lm.total <- lm(total ~ wind, data = NFLclean.train)
lm.line <- lm(total ~ total_line, data = NFLclean.train)
lm.both <- lm(total ~ wind + total_line, data = NFLclean.train)
```


Predict on the test set:
```{r}
pred.total <- predict(lm.total, newdata = NFLclean.test)
pred.line <- predict(lm.line, newdata = NFLclean.test)
pred.both <- predict(lm.both, newdata = NFLclean.test)
```


MSE: 
```{r}
mean((NFLclean.test$total - pred.total)^2)
mean((NFLclean.test$total - pred.line)^2)
mean((NFLclean.test$total - pred.both)^2)
```
Total_line gives the lowest mean squared error (143.0153).

####################################################################################
####################################################################################

4.
Data:
```{r}
evals <- read.csv("https://www.openintro.org/book/statdata/evals.csv")

table(evals$gender)
```
(a)

H0: true difference in means between group female and group male is equal to 0
H1: true difference in means between group female and group male is not equal to 0

```{r}
t.test(score ~ gender, data = evals)
```
P-value is  0.006, which is less than 0.05. 
So, for the alternative hypothesis.
True difference in means between group female and group male is not equal to 0

Female mean = 4.092821	
Male mean = 4.234328


####################################################################################

(b) The relationship between perceived beauty and evaluation score might be different for female and male instructors. 

Subsets:
```{r}
evalsFemale <- subset(evals, gender == "female")
evalsMale <- subset(evals, gender == "male")

lm_f <- lm(score ~ bty_avg, data = evalsFemale)
lm_f

lm_m <- lm(score ~ bty_avg, data = evalsMale)
lm_m
```

Plot the data with beauty on the x-axis and score on the y-axis
  then add both regression lines to the plot, 
  clearly distinguishing which line is which. 

```{r}
plot(evalsFemale$bty_avg, evalsFemale$score, xlab = "Beauty", ylab = "Score", 
     main = "Female: blue; 
     Male: orange")
abline(c(3.95006, 0.03064), col = "blue")
abline(c(3.7666, 0.1103), col = "orange")
```

The orange male line is more steep than female. 
This means that the beauty has a stronger effect on the scores for males than for females participants.


