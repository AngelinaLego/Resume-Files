---
title: "Log-likelihood, Zero-inflated functions" 
author: "Angelina Legostaeva"
date: "2025-03-10"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

###################################################################################

1. plot(density())
2. mixture of two Normal distributions

3. Using a Wald test, test the hypothesis that the two Normal distributions are equally likely

4. Plot the log-likelihood as a function of p, for values of p between 0.5 and 0.7

5. logit <- log(0.6 / (1 - 0.60))
6. 95% profile likelihood confidence interval for logit(p)

7. Negative Binomial
8. Zero-inflated Poisson
9. Poisson
10. Zero-inflated Negative Binomial

###################################################################################


1.  

Can we conclude that there’s a significant difference in goals per game between the two leagues? Perform an appropriate
hypothesis test, based on what you learned from Chapter 3.
Note:
• Both Wald test and likelihood ratio (LR) test would work, although the former might
be easier to understand.
• Review Ch3 slides (pp. 43-47) for the Wald test: COVID example.
• You may study the attached material for the LR test: COVID example.
• Extra credit would be awarded to students who perform both tests.


H0: lambda1 = lambda2
H1: lambda1 not= lambda2

```{r}
eng <- 1071
spain <- 951
n <- 380

obs.diff <- eng/n - spain/n
v <- eng/n^2 + spain/n^2
obs.diff
v

w <- (obs.diff - 0)^2 / v
w
1 - pchisq(w, df = 1)
```
P-value is 0.0076, which is less than 0.05. 
So, I am rejecting the null hypothesis that there is no significance between the goal numbers.
There’s a significant difference in goals per game between the two leagues.



2. 
Data:
```{r}
data(faithful)
eruption <- faithful$eruptions
summary(eruption)

plot(density(eruption), main = "Density Plot for Eruptions")
```

###################################################################################

(a) Using R, model the eruption times as a mixture of two Normal distributions. 

```{r}
#Define the negative log-likelihood
nllhood = function (theta, y) 
  {
  p <- theta [1]
  mu <- theta [2]
  sigma <- theta [ 3]
  nu <- theta [4]
  tau <- theta [5]
  lhood <- p * dnorm(y,mu, sigma) + (1 - p) * dnorm(y, nu, tau)
  return(-sum(log(lhood))) 
}

#Use start values inferred from histogram
f.fit <- optim(c(0.4, 2.0, 0.5, 4.5, 0.5), 
               fn = nllhood, data = eruption, hessian = TRUE)

MLE <- f.fit$par

obs_fisher <- f.fit$hess #Observed Fisher information matrix

v_hat <- solve(obs_fisher) 
SE <- sqrt(diag(v_hat))

#Obtain the MLEs, estimated std errors, and approx Wald 95% CIs
wald_t <- cbind(MLE, SE, LowerBound = MLE - qnorm(0.975) * SE,
UpperBound = MLE + qnorm(0.975) * SE)

parnames <- c ("p", "mu" , "sigma", "nu" , "tau")
rownames (wald_t) <- parnames
round(wald_t, 4) #Print to 4 decimal places.

```

###################################################################################


(b) Using a Wald test, test the hypothesis that the two Normal distributions are equally
likely; i.e. that p = 0.5.
```{r}
er.var <- solve(f.fit$hessian)

w <- (f.fit$par[1] - 0.5)^2 / er.var[1, 1]
1 - pchisq(w, df = 1)
```
###################################################################################


3. In an October 2023 YouGov poll with 1000 respondents, 60% of those surveyed had a favorable
opinion of Taylor Swift. (23% had an unfavorable opinion, while 17% didn’t know.) For the
purpose of this question, treat the 1000 respondents as a random sample of American adults,
and assume that 600 of them said they had a favorable opinion of Taylor Swift.

(a) Find a 95% Wald confidence interval for p:
```{r}
p <- 600/1000
n <- 1000

s <- sqrt( (p * (1-p)) / n)

lower <- p - 1.96 * s
upper <- p + 1.96 * s

c(lower, upper)
```
###################################################################################

(b) Plot the log-likelihood as a function of p, for values of p between 0.5 and 0.7.

```{r}
p <- seq(0.5, 0.7, 0.001)
swift.loglik <- log(dbinom(600, 1000, p))
plot(p, swift.loglik, type = "l", main = "Log-likelihood for Taylor Swift favorability", 
     xlab = "p = proportion favorable to Swift", ylab = "l(p)")


```

###################################################################################

(c) Find a 95% profile likelihood confidence interval for p.
```{r}
library(binom)
binom.profile(600, 1000)
```

###################################################################################

(d) The MLE of logit(p) is 0.41

```{r}
logit <- log(0.6 / (1 - 0.60))
logit
```

(e) Find a 95% profile likelihood confidence interval for logit(p).
```{r}
log(binom.profile(600, 1000)$lower / (1 - binom.profile(600, 1000)$lower))
log(binom.profile(600, 1000)$upper / (1 - binom.profile(600, 1000)$upper))
```

###################################################################################

4.
a) Poisson model, Poi(λ)
The MLE is ˆλ = 20.65.
```{r}

hurricanes <- read.csv("hurricanes(1).csv")
deaths <- hurricanes$alldeaths
lambda.hat <- mean(deaths)
lambda.hat
```


(b) Negative binomial, NB(r, p)
The MLEs are ˆr = 0.446 and ˆp = 0.0211.
```{r}
nbinom.nll <- function(params, data){
size = params[1]
prob = params[2]
lik <- dnbinom(data, size, prob)
loglik <- log(lik)
nll <- -sum(loglik)
return(nll)
}

x.bar <- mean(deaths)
sigma2.hat <- mean(deaths^2) - x.bar^2
p.mom <- x.bar / sigma2.hat
r.mom <- p.mom / (1 - p.mom) * x.bar
deaths.nb.mle <- optim(par = c(r.mom, p.mom),
fn = nbinom.nll,
data = deaths,
hessian = TRUE)
deaths.nb.mle$par
```



(c) Zero-inflated Poisson
The MLEs are ˆp0 = 0.109 and λ = 23.2.
```{r}
pois0.nll <- function(pars, data){
p <- pars[1]
lambda <- pars[2]
pois.lik <- dpois(data, lambda)
real.lik <- rep(NA, length(data))
which0 <- (data == 0)
real.lik[which0 == FALSE] = (1 - p) * pois.lik[which0 == FALSE]
real.lik[which0 == TRUE] = p + (1 - p) * pois.lik[which0 == TRUE]
loglik <- log(real.lik)
return(-sum(loglik))
}
deaths.zip.mle <- optim(par = c(mean(deaths == 0), mean(deaths)),
fn = pois0.nll,
data = deaths)
deaths.zip.mle$par
```

(d) Zero-inflated negative binomial
```{r}
nb0.nll <- function(pars, data){
p <- pars[1]
size <- pars[2]
prob <- pars[3]
nb.lik <- dnbinom(data, size, prob)
real.lik <- rep(NA, length(data))
which0 <- (data == 0)
real.lik[which0 == FALSE] = (1 - p) * nb.lik[which0 == FALSE]
real.lik[which0 == TRUE] = p + (1 - p) * nb.lik[which0 == TRUE]
loglik <- log(real.lik)
return(-sum(loglik))
}
deaths.nb0.mle <- optim(par = c(deaths.zip.mle$par[1],
deaths.nb.mle$par[1],
deaths.nb.mle$par[2]),
fn = nb0.nll,
data = deaths)
deaths.nb0.mle$par
```


The MLE for the probability of an automatic zero is −1.39, which is not a probability.
In fact, the probability of a zero is higher than necessary in the negative binomial model.
So the probability of an automatic zero should be set to 0, and the model reduces to the
ordinary negative binomial model.

